---
title: "Practical Machine Learning Course Project"
author: "Roman Tataurov"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly. 

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

# Question

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. 

# Working environment

Lets load some libraries required and set a pseudo random seed.

```{r}
library(knitr)
opts_chunk$set(echo = TRUE, results = 'hold')
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(missForest)
library(gbm)
library(knitr)
set.seed(1234)
```

# Input data

Lets prepare common function to download an remember our source data. While import files also cleaning data from some misformatted values which obvioulsy mean absense of value (``NA``, ``#DIV/0!`` and just an empty cells).

```{r}
dir.create("./data", showWarnings = FALSE, recursive = TRUE, mode = "0777")

loadData <- function(type) {
  filename=paste("./data/", "pml-", type, ".csv", sep="")
  if (!file.exists(filename)) {
    url <- paste("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-", type ,".csv", sep="")
    message(paste("Please Wait! Downloading...", url, "..."))
    download.file(url, destfile=filename)
  } 
  data = read.csv(filename, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
  return(data)
}
```

Well - now lets load our data and prepare ``pmlTraining`` and ``pmlTesting`` datasets

```{r}
pmlTraining <- loadData("training")
pmlTesting  <- loadData("testing")
```

Lets briefly review our ``pmlTraining`` dataset

```{r}
dim(pmlTraining) 
```

So we have 160 columns so it mean 159 predictors - pretty big value. So its a good reason to think about some data cleaning. 

# Prepare test data

## Split our source training dataset

Well now time to move close to research so lets split our ``training`` dataset using common 60%/40% rule. 

```{r}
inTrain <- createDataPartition(y=pmlTraining$classe, p=0.6, list=FALSE)
training <- pmlTraining[inTrain, ]
testing  <- pmlTraining[-inTrain, ]
dim(training); 
dim(testing)
```

## Cleanup training data

### Remove columns with lot of NA

Lets remove variables with more than 80% of NA values.  

```{r}
filledColumns = c((colSums(!is.na(training[,-ncol(training)])) >= 0.8*nrow(training)))
training <- training[, filledColumns]
testing  <- testing[, filledColumns]
dim(training)
dim(testing)
```

So as result we got 60 variables instead of 160

### Remove near zero values

Lets find covariance with variability close to zero and remove it.

```{r}
nzv <- nearZeroVar(training, saveMetrics=TRUE)
training = subset(training, select=rownames(nzv[which(nzv$nzv==FALSE & nzv$zeroVar==FALSE),]))
dim(training)
```

Not a big advantage because now we have 59 variables instead of 60 

### Remove some known variable

Lets remove timestamp variables as well as user names and identifiers.

```{r}
training = subset(training, select=c(-X -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp))
training = subset(training, select=c(-user_name))
training = subset(training, select=c(-raw_timestamp_part_1))
training = subset(training, select=c(-X))
dim(training)
```

So finally we have only 56 predictors
Now we can assume our training dataset clear and ready for later analysis 

# Building models

## Decision Tree

```{r}
training_DT <- training
modFit_DT <- rpart(classe~., data=training_DT, method="class")
prediction_DT <- predict(modFit_DT, testing, type="class")
cm_DT <- confusionMatrix(prediction_DT, testing$classe)
cm_DT
```

## Random Forests

Lets try Random Forests apprach without some preprocessing 

```{r}
modFit_RF1 <- randomForest(classe~., data=training)
prediction_RF1 <- predict(modFit_RF1, testing)
cm_RF1 <- confusionMatrix(prediction_RF1, testing$classe)
cm_RF1
```

Previusly we already removed columns with more than 80% of NA values. Lets impute missing values with ``missForest`` function.

```{r}
training_RF <- missForest(training, ntree=100)
modFit_RF2 <- randomForest(classe~., data=training_RF$ximp)
prediction_RF2 <- predict(modFit_RF2, testing)
cm_RF2 <- confusionMatrix(prediction_RF2, testing$classe)
cm_RF2
```

Imputing results are comparable but finally worse a little 0.9963 vs 09967 accuracy corrspondingly for imputet and raw data.

# Conclusions

So we can see what Decision Tree algorithm has worse accuracy than Random Forests - 0.7239 vs 0.9967 correspondingly. So our final decision is Random Forests algorithm without impute. 

```{r, echo=FALSE, results="asis"}
modelsSummary = data.frame(
  m=c("Desision Tree", "Random Forest", "Random Forest with impute"), 
  r=c(round(cm_DT$overall[1],3), round(cm_RF1$overall[1],3), round(cm_RF2$overall[1],3)))
kable(modelsSummary, col.names=c("Model", "Accuracy"))
```

# Generate files for submittion

```{r}
prediction <- predict(modFit_RF1, pmlTesting, type="class")
as.vector(prediction)

dir.create("./submissions", showWarnings = FALSE, recursive = TRUE, mode = "0777")
n <- length(prediction)
for(i in 1:n){
  filename <- paste0("./submissions/problem_id_",i,".txt")
  write.table(prediction[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
```
